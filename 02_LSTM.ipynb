{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ca7f3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import re\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ba591cb",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Size of the train dataset: (41157, 6)\nSize of the test dataset: (3798, 6)\n"
     ]
    }
   ],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "ENGLISH_WORDS = set(words.words())\n",
    "df_train = pd.read_csv(r\"data\\Corona_NLP_train.csv\", encoding='latin1')\n",
    "df_test = pd.read_csv(r\"data\\Corona_NLP_test.csv\", encoding='latin1')\n",
    "\n",
    "print(\"Size of the train dataset: {}\".format(df_train.shape))\n",
    "print(\"Size of the test dataset: {}\".format(df_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b82416",
   "metadata": {},
   "source": [
    "I'm defining preprocessing functions from previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13549418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recode_sentiment(y):\n",
    "\n",
    "    if y in ['Extremely Positive', 'Positive']:\n",
    "        return 'Positive'\n",
    "    elif y in ['Extremely Negative', 'Negative']:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "def remove_url(string):\n",
    "    return re.sub(r'https?://\\S+|www\\.\\S+', '', string)\n",
    "\n",
    "def remove_html(string):\n",
    "    return re.sub(r'<.*?>', '', string)\n",
    "\n",
    "def remove_numbers(string):\n",
    "    return re.sub(r'\\d+', '', string)\n",
    "\n",
    "def remove_mentions(string):\n",
    "    return re.sub(r'@\\w+', '', string)\n",
    "\n",
    "def remove_hashtags(string):\n",
    "    return re.sub(r'#\\w+', '', string)\n",
    "\n",
    "def clean_data(tweet, return_tokenized=True):\n",
    "    \n",
    "    # Tokenization\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    cleaned_tweet = []\n",
    "    \n",
    "    for token, tag in pos_tag(tokens):\n",
    "        \n",
    "        # Cleaning tokens with regular expressions\n",
    "        token = remove_url(token)\n",
    "        token = remove_html(token)\n",
    "        token = remove_numbers(token)\n",
    "        token = remove_mentions(token)\n",
    "        token = remove_hashtags(token)\n",
    "        \n",
    "        # Lemmatizing tokens with part of speech recognition\n",
    "        \n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        \n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "        \n",
    "        token = token.lower()\n",
    "        \n",
    "        if token not in punctuation and token not in STOPWORDS and token in ENGLISH_WORDS:\n",
    "            cleaned_tweet.append(token)\n",
    "    #TfidfVectorizer accepts strings instead of lists of tokens\n",
    "    if not return_tokenized:\n",
    "        cleaned_tweet = ' '.join([token for token in cleaned_tweet])\n",
    "\n",
    "    return cleaned_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04e4b7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['OriginalTweet'], df_test['OriginalTweet'] = \\\n",
    "    df_train['OriginalTweet'].apply(lambda x: clean_data(x, return_tokenized=True)),\\\n",
    "    df_test['OriginalTweet'].apply(lambda x: clean_data(x, return_tokenized=True))\n",
    "\n",
    "df_train['Sentiment'], df_test['Sentiment'] = \\\n",
    "    df_train['Sentiment'].apply(recode_sentiment), df_test['Sentiment'].apply(recode_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f88747c",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   UserName  ScreenName                   Location     TweetAt  \\\n",
       "0      3799       48751                     London  16-03-2020   \n",
       "1      3800       48752                         UK  16-03-2020   \n",
       "2      3801       48753                  Vagabonds  16-03-2020   \n",
       "3      3802       48754                        NaN  16-03-2020   \n",
       "4      3803       48755                        NaN  16-03-2020   \n",
       "5      3804       48756  ÃT: 36.319708,-82.363649  16-03-2020   \n",
       "6      3805       48757       35.926541,-78.753267  16-03-2020   \n",
       "7      3806       48758                    Austria  16-03-2020   \n",
       "8      3807       48759            Atlanta, GA USA  16-03-2020   \n",
       "9      3808       48760           BHAVNAGAR,GUJRAT  16-03-2020   \n",
       "\n",
       "                                       OriginalTweet Sentiment  \n",
       "0                                                 []   Neutral  \n",
       "1  [advice, talk, family, exchange, phone, number...  Positive  \n",
       "2  [give, elderly, disable, dedicate, shopping, h...  Positive  \n",
       "3  [food, stock, one, empty, please, panic, enoug...  Positive  \n",
       "4  [ready, go, supermarket, outbreak, paranoid, f...  Negative  \n",
       "5  [news, first, confirm, covid, case, come, coun...  Positive  \n",
       "6  [cashier, grocery, store, share, insight, prov...  Positive  \n",
       "7           [supermarket, today, buy, toilet, paper]   Neutral  \n",
       "8  [due, covid, retail, store, classroom, open, b...  Positive  \n",
       "9  [corona, prevention, stop, buy, thing, cash, u...  Negative  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>UserName</th>\n      <th>ScreenName</th>\n      <th>Location</th>\n      <th>TweetAt</th>\n      <th>OriginalTweet</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3799</td>\n      <td>48751</td>\n      <td>London</td>\n      <td>16-03-2020</td>\n      <td>[]</td>\n      <td>Neutral</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3800</td>\n      <td>48752</td>\n      <td>UK</td>\n      <td>16-03-2020</td>\n      <td>[advice, talk, family, exchange, phone, number...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3801</td>\n      <td>48753</td>\n      <td>Vagabonds</td>\n      <td>16-03-2020</td>\n      <td>[give, elderly, disable, dedicate, shopping, h...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3802</td>\n      <td>48754</td>\n      <td>NaN</td>\n      <td>16-03-2020</td>\n      <td>[food, stock, one, empty, please, panic, enoug...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3803</td>\n      <td>48755</td>\n      <td>NaN</td>\n      <td>16-03-2020</td>\n      <td>[ready, go, supermarket, outbreak, paranoid, f...</td>\n      <td>Negative</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3804</td>\n      <td>48756</td>\n      <td>ÃT: 36.319708,-82.363649</td>\n      <td>16-03-2020</td>\n      <td>[news, first, confirm, covid, case, come, coun...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>3805</td>\n      <td>48757</td>\n      <td>35.926541,-78.753267</td>\n      <td>16-03-2020</td>\n      <td>[cashier, grocery, store, share, insight, prov...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>3806</td>\n      <td>48758</td>\n      <td>Austria</td>\n      <td>16-03-2020</td>\n      <td>[supermarket, today, buy, toilet, paper]</td>\n      <td>Neutral</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>3807</td>\n      <td>48759</td>\n      <td>Atlanta, GA USA</td>\n      <td>16-03-2020</td>\n      <td>[due, covid, retail, store, classroom, open, b...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>3808</td>\n      <td>48760</td>\n      <td>BHAVNAGAR,GUJRAT</td>\n      <td>16-03-2020</td>\n      <td>[corona, prevention, stop, buy, thing, cash, u...</td>\n      <td>Negative</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc6950da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['TweetLen'], df_test['TweetLen'] = \\\n",
    "    df_train['OriginalTweet'].apply(lambda x: len(x)), df_test['OriginalTweet'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ddd9ffc",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   UserName  ScreenName                   Location     TweetAt  \\\n",
       "0      3799       48751                     London  16-03-2020   \n",
       "1      3800       48752                         UK  16-03-2020   \n",
       "2      3801       48753                  Vagabonds  16-03-2020   \n",
       "3      3802       48754                        NaN  16-03-2020   \n",
       "4      3803       48755                        NaN  16-03-2020   \n",
       "5      3804       48756  ÃT: 36.319708,-82.363649  16-03-2020   \n",
       "6      3805       48757       35.926541,-78.753267  16-03-2020   \n",
       "7      3806       48758                    Austria  16-03-2020   \n",
       "8      3807       48759            Atlanta, GA USA  16-03-2020   \n",
       "9      3808       48760           BHAVNAGAR,GUJRAT  16-03-2020   \n",
       "\n",
       "                                       OriginalTweet Sentiment  TweetLen  \n",
       "0                                                 []   Neutral         0  \n",
       "1  [advice, talk, family, exchange, phone, number...  Positive        22  \n",
       "2  [give, elderly, disable, dedicate, shopping, h...  Positive         9  \n",
       "3  [food, stock, one, empty, please, panic, enoug...  Positive        15  \n",
       "4  [ready, go, supermarket, outbreak, paranoid, f...  Negative        14  \n",
       "5  [news, first, confirm, covid, case, come, coun...  Positive        22  \n",
       "6  [cashier, grocery, store, share, insight, prov...  Positive        12  \n",
       "7           [supermarket, today, buy, toilet, paper]   Neutral         5  \n",
       "8  [due, covid, retail, store, classroom, open, b...  Positive        20  \n",
       "9  [corona, prevention, stop, buy, thing, cash, u...  Negative        19  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>UserName</th>\n      <th>ScreenName</th>\n      <th>Location</th>\n      <th>TweetAt</th>\n      <th>OriginalTweet</th>\n      <th>Sentiment</th>\n      <th>TweetLen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3799</td>\n      <td>48751</td>\n      <td>London</td>\n      <td>16-03-2020</td>\n      <td>[]</td>\n      <td>Neutral</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3800</td>\n      <td>48752</td>\n      <td>UK</td>\n      <td>16-03-2020</td>\n      <td>[advice, talk, family, exchange, phone, number...</td>\n      <td>Positive</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3801</td>\n      <td>48753</td>\n      <td>Vagabonds</td>\n      <td>16-03-2020</td>\n      <td>[give, elderly, disable, dedicate, shopping, h...</td>\n      <td>Positive</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3802</td>\n      <td>48754</td>\n      <td>NaN</td>\n      <td>16-03-2020</td>\n      <td>[food, stock, one, empty, please, panic, enoug...</td>\n      <td>Positive</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3803</td>\n      <td>48755</td>\n      <td>NaN</td>\n      <td>16-03-2020</td>\n      <td>[ready, go, supermarket, outbreak, paranoid, f...</td>\n      <td>Negative</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3804</td>\n      <td>48756</td>\n      <td>ÃT: 36.319708,-82.363649</td>\n      <td>16-03-2020</td>\n      <td>[news, first, confirm, covid, case, come, coun...</td>\n      <td>Positive</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>3805</td>\n      <td>48757</td>\n      <td>35.926541,-78.753267</td>\n      <td>16-03-2020</td>\n      <td>[cashier, grocery, store, share, insight, prov...</td>\n      <td>Positive</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>3806</td>\n      <td>48758</td>\n      <td>Austria</td>\n      <td>16-03-2020</td>\n      <td>[supermarket, today, buy, toilet, paper]</td>\n      <td>Neutral</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>3807</td>\n      <td>48759</td>\n      <td>Atlanta, GA USA</td>\n      <td>16-03-2020</td>\n      <td>[due, covid, retail, store, classroom, open, b...</td>\n      <td>Positive</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>3808</td>\n      <td>48760</td>\n      <td>BHAVNAGAR,GUJRAT</td>\n      <td>16-03-2020</td>\n      <td>[corona, prevention, stop, buy, thing, cash, u...</td>\n      <td>Negative</td>\n      <td>19</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80d23648",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = \\\n",
    "    df_train.loc[df_train['TweetLen'] > 0,], df_test.loc[df_test['TweetLen'] > 0,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "819e73f7",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    UserName  ScreenName                   Location     TweetAt  \\\n",
       "1       3800       48752                         UK  16-03-2020   \n",
       "2       3801       48753                  Vagabonds  16-03-2020   \n",
       "3       3802       48754                        NaN  16-03-2020   \n",
       "4       3803       48755                        NaN  16-03-2020   \n",
       "5       3804       48756  ÃT: 36.319708,-82.363649  16-03-2020   \n",
       "6       3805       48757       35.926541,-78.753267  16-03-2020   \n",
       "7       3806       48758                    Austria  16-03-2020   \n",
       "8       3807       48759            Atlanta, GA USA  16-03-2020   \n",
       "9       3808       48760           BHAVNAGAR,GUJRAT  16-03-2020   \n",
       "10      3809       48761             Makati, Manila  16-03-2020   \n",
       "\n",
       "                                        OriginalTweet Sentiment  TweetLen  \n",
       "1   [advice, talk, family, exchange, phone, number...  Positive        22  \n",
       "2   [give, elderly, disable, dedicate, shopping, h...  Positive         9  \n",
       "3   [food, stock, one, empty, please, panic, enoug...  Positive        15  \n",
       "4   [ready, go, supermarket, outbreak, paranoid, f...  Negative        14  \n",
       "5   [news, first, confirm, covid, case, come, coun...  Positive        22  \n",
       "6   [cashier, grocery, store, share, insight, prov...  Positive        12  \n",
       "7            [supermarket, today, buy, toilet, paper]   Neutral         5  \n",
       "8   [due, covid, retail, store, classroom, open, b...  Positive        20  \n",
       "9   [corona, prevention, stop, buy, thing, cash, u...  Negative        19  \n",
       "10  [month, crowd, supermarket, restaurant, howeve...   Neutral        16  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>UserName</th>\n      <th>ScreenName</th>\n      <th>Location</th>\n      <th>TweetAt</th>\n      <th>OriginalTweet</th>\n      <th>Sentiment</th>\n      <th>TweetLen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>3800</td>\n      <td>48752</td>\n      <td>UK</td>\n      <td>16-03-2020</td>\n      <td>[advice, talk, family, exchange, phone, number...</td>\n      <td>Positive</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3801</td>\n      <td>48753</td>\n      <td>Vagabonds</td>\n      <td>16-03-2020</td>\n      <td>[give, elderly, disable, dedicate, shopping, h...</td>\n      <td>Positive</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3802</td>\n      <td>48754</td>\n      <td>NaN</td>\n      <td>16-03-2020</td>\n      <td>[food, stock, one, empty, please, panic, enoug...</td>\n      <td>Positive</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3803</td>\n      <td>48755</td>\n      <td>NaN</td>\n      <td>16-03-2020</td>\n      <td>[ready, go, supermarket, outbreak, paranoid, f...</td>\n      <td>Negative</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3804</td>\n      <td>48756</td>\n      <td>ÃT: 36.319708,-82.363649</td>\n      <td>16-03-2020</td>\n      <td>[news, first, confirm, covid, case, come, coun...</td>\n      <td>Positive</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>3805</td>\n      <td>48757</td>\n      <td>35.926541,-78.753267</td>\n      <td>16-03-2020</td>\n      <td>[cashier, grocery, store, share, insight, prov...</td>\n      <td>Positive</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>3806</td>\n      <td>48758</td>\n      <td>Austria</td>\n      <td>16-03-2020</td>\n      <td>[supermarket, today, buy, toilet, paper]</td>\n      <td>Neutral</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>3807</td>\n      <td>48759</td>\n      <td>Atlanta, GA USA</td>\n      <td>16-03-2020</td>\n      <td>[due, covid, retail, store, classroom, open, b...</td>\n      <td>Positive</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>3808</td>\n      <td>48760</td>\n      <td>BHAVNAGAR,GUJRAT</td>\n      <td>16-03-2020</td>\n      <td>[corona, prevention, stop, buy, thing, cash, u...</td>\n      <td>Negative</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>3809</td>\n      <td>48761</td>\n      <td>Makati, Manila</td>\n      <td>16-03-2020</td>\n      <td>[month, crowd, supermarket, restaurant, howeve...</td>\n      <td>Neutral</td>\n      <td>16</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "840d2c73",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Size of the train dataset: (41052, 7)\nSize of the test dataset: (3792, 7)\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of the train dataset: {}\".format(df_train.shape))\n",
    "print(\"Size of the test dataset: {}\".format(df_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91e196c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(data):\n",
    "    all_words = []\n",
    "    for x in data:\n",
    "        for token in x:\n",
    "            all_words.append(token)\n",
    "    \n",
    "    return set(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f497cef2",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocabulary length: 12962\n"
     ]
    }
   ],
   "source": [
    "vocab = create_corpus(df_train['OriginalTweet'].values)\n",
    "print('Vocabulary length: {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionaries(vocab):\n",
    "\n",
    "    word_to_int_dict = {w:i+1 for i, w in enumerate(vocab)}\n",
    "    int_to_word_dict = {i:w for w, i in word_to_int_dict.items()}\n",
    "\n",
    "    word_to_int_dict[''] = 0\n",
    "    int_to_word_dict[0] = ''\n",
    "\n",
    "    return word_to_int_dict, int_to_word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_int_dict, int_to_word_dict = create_dictionaries(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(sequence, target_len=25):\n",
    "\n",
    "    padded_sequence = sequence.copy()\n",
    "\n",
    "    length = len(padded_sequence)\n",
    "\n",
    "    if length > target_len:\n",
    "        padded_sequence = padded_sequence[:target_len]\n",
    "    elif length < target_len:\n",
    "        while length < target_len:\n",
    "            padded_sequence.append('')\n",
    "            length += 1\n",
    "    \n",
    "    return padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "count    41052.000000\n",
       "mean        13.420150\n",
       "std          6.091071\n",
       "min          1.000000\n",
       "25%          8.000000\n",
       "50%         14.000000\n",
       "75%         18.000000\n",
       "max         34.000000\n",
       "Name: TweetLen, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "df_train['TweetLen'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['OriginalTweet'], df_test['OriginalTweet'] = df_train['OriginalTweet'].apply(pad_sequence), df_test['OriginalTweet'].apply(pad_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "count    41052.0\n",
       "mean        25.0\n",
       "std          0.0\n",
       "min         25.0\n",
       "25%         25.0\n",
       "50%         25.0\n",
       "75%         25.0\n",
       "max         25.0\n",
       "Name: TweetLen, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "df_train['TweetLen'], df_test['TweetLen'] = \\\n",
    "    df_train['OriginalTweet'].apply(lambda x: len(x)), df_test['OriginalTweet'].apply(lambda x: len(x))\n",
    "df_train['TweetLen'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sequence(text):\n",
    "\n",
    "    encoded_sequence = np.array([word_to_int_dict[word] if word in word_to_int_dict.keys() else word_to_int_dict[''] for word in text])\n",
    "    return encoded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original sequence: ['advice', 'talk', 'family', 'exchange', 'phone', 'number', 'create', 'contact', 'list', 'phone', 'number', 'school', 'employer', 'chemist', 'set', 'shopping', 'account', 'poss', 'adequate', 'supply', 'regular', 'order', '', '', '']\n\n--------------------------------------------------\nEncoded sequence: [11504  7963  2636  2163  5358  5926  6490  6069  4950  5358  5926 10242\n  8527  3373 12923    46 12543   367  4531  4138  2567  2137     0     0\n     0]\n"
     ]
    }
   ],
   "source": [
    "print('Original sequence: {}'.format(df_train['OriginalTweet'].iloc[0]))\n",
    "print()\n",
    "print('--'*25)\n",
    "print('Encoded sequence: {}'.format(encode_sequence(df_train['OriginalTweet'].iloc[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mapping_dict = {'Negative':0, 'Neutral':1, 'Positive':2}\n",
    "\n",
    "x_train = []\n",
    "\n",
    "for x in df_train['OriginalTweet'].values:\n",
    "    x_train.append(encode_sequence(x))\n",
    "x_train = np.array(x_train)\n",
    "\n",
    "y_train = df_train['Sentiment'].map(y_mapping_dict).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x_train shape: (41052, 25)\ny_train shape: (41052,)\n"
     ]
    }
   ],
   "source": [
    "print('x_train shape: {}'.format(x_train.shape))\n",
    "print('y_train shape: {}'.format(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_idx_border = int(df_test.shape[0] / 2)\n",
    "\n",
    "x_valid = []\n",
    "x_test = []\n",
    "\n",
    "for x in df_test['OriginalTweet'].values[:valid_idx_border]:\n",
    "    x_valid.append(encode_sequence(x))\n",
    "\n",
    "for x in df_test['OriginalTweet'].values[valid_idx_border:]:\n",
    "    x_test.append(encode_sequence(x))\n",
    "\n",
    "x_valid = np.array(x_valid)\n",
    "x_test = np.array(x_test)\n",
    "\n",
    "y_valid = df_test['Sentiment'].map(y_mapping_dict).values[:valid_idx_border]\n",
    "y_test = df_test['Sentiment'].map(y_mapping_dict).values[valid_idx_border:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x_valid shape: (1896, 25)\ny_valid shape: (1896,)\n--------------------------------------------------\nx_test shape: (1896, 25)\ny_test shape: (1896,)\n"
     ]
    }
   ],
   "source": [
    "print('x_valid shape: {}'.format(x_valid.shape))\n",
    "print('y_valid shape: {}'.format(y_valid.shape))\n",
    "print('-'*50)\n",
    "print('x_test shape: {}'.format(x_test.shape))\n",
    "print('y_test shape: {}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 1774  9623 10785   995  3233     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0]\n--------------------------------------------------\n[ 1734  6453  7130 10396  8286 10396  3073 12698  4387 10396  4095  3043\n 10396  5570     0     0     0     0     0     0     0     0     0     0\n     0]\n"
     ]
    }
   ],
   "source": [
    "print(x_valid[-1])\n",
    "print('-'*50)\n",
    "print(x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda:0\n_CudaDeviceProperties(name='GeForce RTX 2070 SUPER', major=7, minor=5, total_memory=8192MB, multi_processor_count=40)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "print(torch.cuda.get_device_properties('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = torch.tensor(x_train, device=device).long(), torch.tensor(y_train, device=device).long()\n",
    "valid_x, valid_y = torch.tensor(x_valid, device=device).long(), torch.tensor(y_valid, device=device).long()\n",
    "test_x, test_y = torch.tensor(x_test, device=device).long(), torch.tensor(y_test, device=device).long()\n",
    "\n",
    "train_data = TensorDataset(train_x, train_y)\n",
    "valid_data = TensorDataset(valid_x, valid_y)\n",
    "test_data = TensorDataset(test_x, test_y)\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sample input: \ntensor([[12948,  4534,  6402,  1641,  1456, 11835, 11759,  9635,  6195, 12687,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0]], device='cuda:0')\ntorch.Size([1, 25])\nSample label: \ntensor([1], device='cuda:0')\ntorch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "data_iter = iter(train_loader)\n",
    "sample_x, sample_y = next(data_iter)\n",
    "\n",
    "print('Sample input: ')\n",
    "print(sample_x)\n",
    "print(sample_x.size())\n",
    "print('Sample label: ')\n",
    "print(sample_y)\n",
    "print(sample_y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, n_vocab, n_embed, n_hidden, n_output, n_layers, drop_p = 0.8):\n",
    "\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "\n",
    "        self.n_vocab = n_vocab\n",
    "        self.n_embed = n_embed\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        self.n_layers = n_layers\n",
    "        self.drop_p = drop_p\n",
    "\n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        self.lstm = nn.LSTM(n_embed, n_hidden, n_layers, batch_first = True, dropout = drop_p)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.fc = nn.Linear(n_hidden, n_output)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        embedded_words = self.embedding(x)\n",
    "        lstm_out, h = self.lstm(embedded_words)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        fc_out = self.fc(lstm_out)\n",
    "        softmax_out = self.softmax(fc_out)\n",
    "        softmax_out = softmax_out.view(batch_size, -1)\n",
    "        softmax_last_three = softmax_out[:, -3:]\n",
    "\n",
    "        return softmax_last_three, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "\n",
    "        device = torch.device('cuda:0')\n",
    "        weights = next(self.parameters()).data\n",
    "        h = (weights.new(self.n_layers, batch_size,\\\n",
    "        self.n_hidden).zero_().to(device),\\\n",
    "        weights.new(self.n_layers, batch_size,\\\n",
    "        self.n_hidden).zero_().to(device))\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "SentimentLSTM(\n",
       "  (embedding): Embedding(12963, 100)\n",
       "  (lstm): LSTM(100, 50, num_layers=2, batch_first=True, dropout=0.8)\n",
       "  (dropout): Dropout(p=0.8, inplace=False)\n",
       "  (fc): Linear(in_features=50, out_features=3, bias=True)\n",
       "  (softmax): LogSoftmax(dim=-1)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "source": [
    "n_vocab = len(word_to_int_dict)\n",
    "n_embed = 100\n",
    "n_hidden = 50\n",
    "n_output = 3\n",
    "n_layers = 2\n",
    "\n",
    "net = SentimentLSTM(n_vocab, n_embed, n_hidden, n_output, n_layers)\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every = 500\n",
    "step = 0\n",
    "n_epochs = 3\n",
    "clip = 5 \n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.RMSprop(net.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-105-7142cd5b2e55>:11: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(net.parameters(), clip)\n",
      "Epoch: 1/3 Step: 500 Training Loss: 0.7053 Validation Loss: 1.0346\n",
      "Epoch: 1/3 Step: 1000 Training Loss: 1.0703 Validation Loss: 1.0208\n",
      "Epoch: 1/3 Step: 1500 Training Loss: 0.9957 Validation Loss: 1.0091\n",
      "Epoch: 1/3 Step: 2000 Training Loss: 0.3967 Validation Loss: 0.9939\n",
      "Epoch: 1/3 Step: 2500 Training Loss: 2.2593 Validation Loss: 0.9781\n",
      "Epoch: 1/3 Step: 3000 Training Loss: 1.5097 Validation Loss: 0.9621\n",
      "Epoch: 1/3 Step: 3500 Training Loss: 0.7101 Validation Loss: 0.9673\n",
      "Epoch: 1/3 Step: 4000 Training Loss: 1.0625 Validation Loss: 0.9778\n",
      "Epoch: 1/3 Step: 4500 Training Loss: 3.9474 Validation Loss: 0.9677\n",
      "Epoch: 1/3 Step: 5000 Training Loss: 0.9658 Validation Loss: 0.9664\n",
      "Epoch: 1/3 Step: 5500 Training Loss: 1.1899 Validation Loss: 0.9711\n",
      "Epoch: 1/3 Step: 6000 Training Loss: 1.0946 Validation Loss: 0.9776\n",
      "Epoch: 1/3 Step: 6500 Training Loss: 0.9448 Validation Loss: 0.9807\n",
      "Epoch: 1/3 Step: 7000 Training Loss: 2.0116 Validation Loss: 0.9778\n",
      "Epoch: 1/3 Step: 7500 Training Loss: 0.7988 Validation Loss: 0.9704\n",
      "Epoch: 1/3 Step: 8000 Training Loss: 1.3468 Validation Loss: 0.9555\n",
      "Epoch: 1/3 Step: 8500 Training Loss: 1.0400 Validation Loss: 0.9417\n",
      "Epoch: 1/3 Step: 9000 Training Loss: 0.7175 Validation Loss: 0.9551\n",
      "Epoch: 1/3 Step: 9500 Training Loss: 1.2730 Validation Loss: 0.9677\n",
      "Epoch: 1/3 Step: 10000 Training Loss: 0.8466 Validation Loss: 0.9828\n",
      "Epoch: 1/3 Step: 10500 Training Loss: 0.9606 Validation Loss: 0.9452\n",
      "Epoch: 1/3 Step: 11000 Training Loss: 3.9890 Validation Loss: 0.9349\n",
      "Epoch: 1/3 Step: 11500 Training Loss: 2.8159 Validation Loss: 0.9524\n",
      "Epoch: 1/3 Step: 12000 Training Loss: 2.4223 Validation Loss: 0.9307\n",
      "Epoch: 1/3 Step: 12500 Training Loss: 0.6004 Validation Loss: 0.9264\n",
      "Epoch: 1/3 Step: 13000 Training Loss: 0.6439 Validation Loss: 0.9770\n",
      "Epoch: 1/3 Step: 13500 Training Loss: 0.8527 Validation Loss: 0.9325\n",
      "Epoch: 1/3 Step: 14000 Training Loss: 0.6483 Validation Loss: 0.9248\n",
      "Epoch: 1/3 Step: 14500 Training Loss: 2.1061 Validation Loss: 0.9196\n",
      "Epoch: 1/3 Step: 15000 Training Loss: 2.9232 Validation Loss: 0.9663\n",
      "Epoch: 1/3 Step: 15500 Training Loss: 0.9425 Validation Loss: 0.9202\n",
      "Epoch: 1/3 Step: 16000 Training Loss: 0.9289 Validation Loss: 0.9401\n",
      "Epoch: 1/3 Step: 16500 Training Loss: 0.6777 Validation Loss: 0.8862\n",
      "Epoch: 1/3 Step: 17000 Training Loss: 1.1343 Validation Loss: 0.9090\n",
      "Epoch: 1/3 Step: 17500 Training Loss: 0.9723 Validation Loss: 0.9484\n",
      "Epoch: 1/3 Step: 18000 Training Loss: 0.5958 Validation Loss: 0.8805\n",
      "Epoch: 1/3 Step: 18500 Training Loss: 0.2157 Validation Loss: 0.8799\n",
      "Epoch: 1/3 Step: 19000 Training Loss: 2.9353 Validation Loss: 0.9270\n",
      "Epoch: 1/3 Step: 19500 Training Loss: 3.4189 Validation Loss: 0.9030\n",
      "Epoch: 1/3 Step: 20000 Training Loss: 0.0141 Validation Loss: 1.0341\n",
      "Epoch: 1/3 Step: 20500 Training Loss: 1.0764 Validation Loss: 0.9486\n",
      "Epoch: 1/3 Step: 21000 Training Loss: 1.3220 Validation Loss: 0.9692\n",
      "Epoch: 1/3 Step: 21500 Training Loss: 0.1587 Validation Loss: 0.8404\n",
      "Epoch: 1/3 Step: 22000 Training Loss: 0.0947 Validation Loss: 0.9451\n",
      "Epoch: 1/3 Step: 22500 Training Loss: 0.1149 Validation Loss: 0.9007\n",
      "Epoch: 1/3 Step: 23000 Training Loss: 0.0187 Validation Loss: 1.0203\n",
      "Epoch: 1/3 Step: 23500 Training Loss: 0.4038 Validation Loss: 0.8802\n",
      "Epoch: 1/3 Step: 24000 Training Loss: 0.1487 Validation Loss: 0.8709\n",
      "Epoch: 1/3 Step: 24500 Training Loss: 5.2878 Validation Loss: 0.9482\n",
      "Epoch: 1/3 Step: 25000 Training Loss: 0.0723 Validation Loss: 0.9280\n",
      "Epoch: 1/3 Step: 25500 Training Loss: 1.1003 Validation Loss: 0.8647\n",
      "Epoch: 1/3 Step: 26000 Training Loss: 0.0602 Validation Loss: 0.9530\n",
      "Epoch: 1/3 Step: 26500 Training Loss: 0.4294 Validation Loss: 0.8916\n",
      "Epoch: 1/3 Step: 27000 Training Loss: 2.1717 Validation Loss: 0.8650\n",
      "Epoch: 1/3 Step: 27500 Training Loss: 0.0083 Validation Loss: 0.9294\n",
      "Epoch: 1/3 Step: 28000 Training Loss: 1.7099 Validation Loss: 0.9432\n",
      "Epoch: 1/3 Step: 28500 Training Loss: 0.9842 Validation Loss: 0.9415\n",
      "Epoch: 1/3 Step: 29000 Training Loss: 0.4365 Validation Loss: 0.8595\n",
      "Epoch: 1/3 Step: 29500 Training Loss: 1.5972 Validation Loss: 0.8132\n",
      "Epoch: 1/3 Step: 30000 Training Loss: 1.2985 Validation Loss: 0.8579\n",
      "Epoch: 1/3 Step: 30500 Training Loss: 0.3510 Validation Loss: 0.8474\n",
      "Epoch: 1/3 Step: 31000 Training Loss: 0.3030 Validation Loss: 0.8757\n",
      "Epoch: 1/3 Step: 31500 Training Loss: 0.2733 Validation Loss: 0.9314\n",
      "Epoch: 1/3 Step: 32000 Training Loss: 1.2958 Validation Loss: 0.8183\n",
      "Epoch: 1/3 Step: 32500 Training Loss: 0.0115 Validation Loss: 0.8712\n",
      "Epoch: 1/3 Step: 33000 Training Loss: 0.3568 Validation Loss: 0.8650\n",
      "Epoch: 1/3 Step: 33500 Training Loss: 3.5221 Validation Loss: 0.8863\n",
      "Epoch: 1/3 Step: 34000 Training Loss: 1.4230 Validation Loss: 0.8557\n",
      "Epoch: 1/3 Step: 34500 Training Loss: 0.0389 Validation Loss: 0.8406\n",
      "Epoch: 1/3 Step: 35000 Training Loss: 4.3560 Validation Loss: 0.8370\n",
      "Epoch: 1/3 Step: 35500 Training Loss: 0.6264 Validation Loss: 0.8567\n",
      "Epoch: 1/3 Step: 36000 Training Loss: 0.9750 Validation Loss: 0.8370\n",
      "Epoch: 1/3 Step: 36500 Training Loss: 1.1168 Validation Loss: 0.8411\n",
      "Epoch: 1/3 Step: 37000 Training Loss: 0.2300 Validation Loss: 0.8165\n",
      "Epoch: 1/3 Step: 37500 Training Loss: 0.1219 Validation Loss: 0.8723\n",
      "Epoch: 1/3 Step: 38000 Training Loss: 1.3176 Validation Loss: 0.8781\n",
      "Epoch: 1/3 Step: 38500 Training Loss: 0.1430 Validation Loss: 0.8134\n",
      "Epoch: 1/3 Step: 39000 Training Loss: 0.0471 Validation Loss: 0.7860\n",
      "Epoch: 1/3 Step: 39500 Training Loss: 0.3037 Validation Loss: 0.9048\n",
      "Epoch: 1/3 Step: 40000 Training Loss: 1.8177 Validation Loss: 0.7643\n",
      "Epoch: 1/3 Step: 40500 Training Loss: 0.0000 Validation Loss: 0.8387\n",
      "Epoch: 1/3 Step: 41000 Training Loss: 0.5438 Validation Loss: 0.8584\n",
      "Epoch: 2/3 Step: 41500 Training Loss: 0.1707 Validation Loss: 0.9451\n",
      "Epoch: 2/3 Step: 42000 Training Loss: 0.2456 Validation Loss: 0.9205\n",
      "Epoch: 2/3 Step: 42500 Training Loss: 0.0073 Validation Loss: 0.8487\n",
      "Epoch: 2/3 Step: 43000 Training Loss: 0.0048 Validation Loss: 0.8464\n",
      "Epoch: 2/3 Step: 43500 Training Loss: 0.1125 Validation Loss: 0.8677\n",
      "Epoch: 2/3 Step: 44000 Training Loss: 0.4808 Validation Loss: 0.8613\n",
      "Epoch: 2/3 Step: 44500 Training Loss: 0.0046 Validation Loss: 0.8129\n",
      "Epoch: 2/3 Step: 45000 Training Loss: 0.4763 Validation Loss: 0.8816\n",
      "Epoch: 2/3 Step: 45500 Training Loss: 1.2517 Validation Loss: 0.8238\n",
      "Epoch: 2/3 Step: 46000 Training Loss: 0.0241 Validation Loss: 0.8928\n",
      "Epoch: 2/3 Step: 46500 Training Loss: 0.7398 Validation Loss: 0.8976\n",
      "Epoch: 2/3 Step: 47000 Training Loss: 1.0596 Validation Loss: 0.7801\n",
      "Epoch: 2/3 Step: 47500 Training Loss: 1.4758 Validation Loss: 0.8094\n",
      "Epoch: 2/3 Step: 48000 Training Loss: 0.0005 Validation Loss: 0.9095\n",
      "Epoch: 2/3 Step: 48500 Training Loss: 0.3731 Validation Loss: 0.9023\n",
      "Epoch: 2/3 Step: 49000 Training Loss: 0.0048 Validation Loss: 0.8814\n",
      "Epoch: 2/3 Step: 49500 Training Loss: 0.0438 Validation Loss: 0.8168\n",
      "Epoch: 2/3 Step: 50000 Training Loss: 0.0481 Validation Loss: 0.7331\n",
      "Epoch: 2/3 Step: 50500 Training Loss: 0.1852 Validation Loss: 0.8102\n",
      "Epoch: 2/3 Step: 51000 Training Loss: 1.3454 Validation Loss: 0.8265\n",
      "Epoch: 2/3 Step: 51500 Training Loss: 1.3363 Validation Loss: 0.7756\n",
      "Epoch: 2/3 Step: 52000 Training Loss: 0.0703 Validation Loss: 0.7614\n",
      "Epoch: 2/3 Step: 52500 Training Loss: 0.0011 Validation Loss: 0.8273\n",
      "Epoch: 2/3 Step: 53000 Training Loss: 0.0092 Validation Loss: 0.8143\n",
      "Epoch: 2/3 Step: 53500 Training Loss: 0.1022 Validation Loss: 0.7585\n",
      "Epoch: 2/3 Step: 54000 Training Loss: 0.1270 Validation Loss: 0.7925\n",
      "Epoch: 2/3 Step: 54500 Training Loss: 0.0835 Validation Loss: 0.7883\n",
      "Epoch: 2/3 Step: 55000 Training Loss: 1.2308 Validation Loss: 0.7310\n",
      "Epoch: 2/3 Step: 55500 Training Loss: 0.0993 Validation Loss: 0.7642\n",
      "Epoch: 2/3 Step: 56000 Training Loss: 0.0001 Validation Loss: 0.7693\n",
      "Epoch: 2/3 Step: 56500 Training Loss: 0.0012 Validation Loss: 0.8196\n",
      "Epoch: 2/3 Step: 57000 Training Loss: 1.7847 Validation Loss: 0.8123\n",
      "Epoch: 2/3 Step: 57500 Training Loss: 0.8305 Validation Loss: 0.7848\n",
      "Epoch: 2/3 Step: 58000 Training Loss: 0.0459 Validation Loss: 0.7202\n",
      "Epoch: 2/3 Step: 58500 Training Loss: 0.2527 Validation Loss: 0.8080\n",
      "Epoch: 2/3 Step: 59000 Training Loss: 0.7611 Validation Loss: 0.7833\n",
      "Epoch: 2/3 Step: 59500 Training Loss: 0.3701 Validation Loss: 0.8279\n",
      "Epoch: 2/3 Step: 60000 Training Loss: 0.0092 Validation Loss: 0.8857\n",
      "Epoch: 2/3 Step: 60500 Training Loss: 0.0001 Validation Loss: 0.8680\n",
      "Epoch: 2/3 Step: 61000 Training Loss: 0.9475 Validation Loss: 0.7151\n",
      "Epoch: 2/3 Step: 61500 Training Loss: 1.3717 Validation Loss: 0.7507\n",
      "Epoch: 2/3 Step: 62000 Training Loss: 0.0015 Validation Loss: 0.7932\n",
      "Epoch: 2/3 Step: 62500 Training Loss: 0.7212 Validation Loss: 0.7671\n",
      "Epoch: 2/3 Step: 63000 Training Loss: 0.0052 Validation Loss: 0.7337\n",
      "Epoch: 2/3 Step: 63500 Training Loss: 0.6980 Validation Loss: 0.7388\n",
      "Epoch: 2/3 Step: 64000 Training Loss: 0.4655 Validation Loss: 0.8126\n",
      "Epoch: 2/3 Step: 64500 Training Loss: 0.4919 Validation Loss: 0.7506\n",
      "Epoch: 2/3 Step: 65000 Training Loss: 0.1710 Validation Loss: 0.7548\n",
      "Epoch: 2/3 Step: 65500 Training Loss: 0.0008 Validation Loss: 0.8606\n",
      "Epoch: 2/3 Step: 66000 Training Loss: 0.4714 Validation Loss: 0.8872\n",
      "Epoch: 2/3 Step: 66500 Training Loss: 0.2738 Validation Loss: 0.8336\n",
      "Epoch: 2/3 Step: 67000 Training Loss: 0.6019 Validation Loss: 0.8453\n",
      "Epoch: 2/3 Step: 67500 Training Loss: 1.9192 Validation Loss: 0.7749\n",
      "Epoch: 2/3 Step: 68000 Training Loss: 9.6192 Validation Loss: 0.7236\n",
      "Epoch: 2/3 Step: 68500 Training Loss: 3.4100 Validation Loss: 0.7466\n",
      "Epoch: 2/3 Step: 69000 Training Loss: 3.7683 Validation Loss: 0.7350\n",
      "Epoch: 2/3 Step: 69500 Training Loss: 0.2278 Validation Loss: 0.7390\n",
      "Epoch: 2/3 Step: 70000 Training Loss: 0.4038 Validation Loss: 0.7356\n",
      "Epoch: 2/3 Step: 70500 Training Loss: 0.0075 Validation Loss: 0.7763\n",
      "Epoch: 2/3 Step: 71000 Training Loss: 0.7639 Validation Loss: 0.9043\n",
      "Epoch: 2/3 Step: 71500 Training Loss: 0.0143 Validation Loss: 0.7654\n",
      "Epoch: 2/3 Step: 72000 Training Loss: 1.3004 Validation Loss: 0.7936\n",
      "Epoch: 2/3 Step: 72500 Training Loss: 0.0960 Validation Loss: 0.8473\n",
      "Epoch: 2/3 Step: 73000 Training Loss: 0.1509 Validation Loss: 0.8440\n",
      "Epoch: 2/3 Step: 73500 Training Loss: 0.2130 Validation Loss: 0.7466\n",
      "Epoch: 2/3 Step: 74000 Training Loss: 0.0032 Validation Loss: 0.7374\n",
      "Epoch: 2/3 Step: 74500 Training Loss: 0.0045 Validation Loss: 0.8779\n",
      "Epoch: 2/3 Step: 75000 Training Loss: 2.7971 Validation Loss: 0.8228\n",
      "Epoch: 2/3 Step: 75500 Training Loss: 0.0012 Validation Loss: 0.8983\n",
      "Epoch: 2/3 Step: 76000 Training Loss: 1.1653 Validation Loss: 0.7388\n",
      "Epoch: 2/3 Step: 76500 Training Loss: 0.1352 Validation Loss: 0.7207\n",
      "Epoch: 2/3 Step: 77000 Training Loss: 0.2649 Validation Loss: 0.6954\n",
      "Epoch: 2/3 Step: 77500 Training Loss: 0.2357 Validation Loss: 0.7702\n",
      "Epoch: 2/3 Step: 78000 Training Loss: 0.4979 Validation Loss: 0.8139\n",
      "Epoch: 2/3 Step: 78500 Training Loss: 0.0004 Validation Loss: 0.8431\n",
      "Epoch: 2/3 Step: 79000 Training Loss: 2.3180 Validation Loss: 0.7398\n",
      "Epoch: 2/3 Step: 79500 Training Loss: 0.0130 Validation Loss: 0.7856\n",
      "Epoch: 2/3 Step: 80000 Training Loss: 1.1296 Validation Loss: 0.7753\n",
      "Epoch: 2/3 Step: 80500 Training Loss: 1.1973 Validation Loss: 0.8282\n",
      "Epoch: 2/3 Step: 81000 Training Loss: 0.1755 Validation Loss: 0.8030\n",
      "Epoch: 2/3 Step: 81500 Training Loss: 0.1680 Validation Loss: 0.7300\n",
      "Epoch: 2/3 Step: 82000 Training Loss: 0.1107 Validation Loss: 0.8240\n",
      "Epoch: 3/3 Step: 82500 Training Loss: 0.0008 Validation Loss: 0.7405\n",
      "Epoch: 3/3 Step: 83000 Training Loss: 0.4909 Validation Loss: 0.7705\n",
      "Epoch: 3/3 Step: 83500 Training Loss: 4.6064 Validation Loss: 0.7704\n",
      "Epoch: 3/3 Step: 84000 Training Loss: 0.0000 Validation Loss: 0.8630\n",
      "Epoch: 3/3 Step: 84500 Training Loss: 0.0040 Validation Loss: 0.8364\n",
      "Epoch: 3/3 Step: 85000 Training Loss: 0.0047 Validation Loss: 0.9452\n",
      "Epoch: 3/3 Step: 85500 Training Loss: 0.0517 Validation Loss: 0.9429\n",
      "Epoch: 3/3 Step: 86000 Training Loss: 1.1570 Validation Loss: 0.8177\n",
      "Epoch: 3/3 Step: 86500 Training Loss: 0.2694 Validation Loss: 0.9150\n",
      "Epoch: 3/3 Step: 87000 Training Loss: 0.0000 Validation Loss: 1.0090\n",
      "Epoch: 3/3 Step: 87500 Training Loss: 0.0002 Validation Loss: 1.0292\n",
      "Epoch: 3/3 Step: 88000 Training Loss: 0.0446 Validation Loss: 0.7903\n",
      "Epoch: 3/3 Step: 88500 Training Loss: 0.0296 Validation Loss: 1.0431\n",
      "Epoch: 3/3 Step: 89000 Training Loss: 0.0004 Validation Loss: 0.9671\n",
      "Epoch: 3/3 Step: 89500 Training Loss: 3.1526 Validation Loss: 0.8319\n",
      "Epoch: 3/3 Step: 90000 Training Loss: 0.0023 Validation Loss: 0.9026\n",
      "Epoch: 3/3 Step: 90500 Training Loss: 0.0018 Validation Loss: 0.7752\n",
      "Epoch: 3/3 Step: 91000 Training Loss: 0.0517 Validation Loss: 0.7962\n",
      "Epoch: 3/3 Step: 91500 Training Loss: 0.0000 Validation Loss: 0.7731\n",
      "Epoch: 3/3 Step: 92000 Training Loss: 0.5043 Validation Loss: 0.8553\n",
      "Epoch: 3/3 Step: 92500 Training Loss: 0.0000 Validation Loss: 0.8774\n",
      "Epoch: 3/3 Step: 93000 Training Loss: 0.0152 Validation Loss: 0.9056\n",
      "Epoch: 3/3 Step: 93500 Training Loss: 0.6731 Validation Loss: 0.8808\n",
      "Epoch: 3/3 Step: 94000 Training Loss: 0.0000 Validation Loss: 0.8534\n",
      "Epoch: 3/3 Step: 94500 Training Loss: 0.5528 Validation Loss: 0.8116\n",
      "Epoch: 3/3 Step: 95000 Training Loss: 0.2377 Validation Loss: 0.8393\n",
      "Epoch: 3/3 Step: 95500 Training Loss: 0.3323 Validation Loss: 0.7360\n",
      "Epoch: 3/3 Step: 96000 Training Loss: 0.0000 Validation Loss: 0.8369\n",
      "Epoch: 3/3 Step: 96500 Training Loss: 0.0061 Validation Loss: 0.7503\n",
      "Epoch: 3/3 Step: 97000 Training Loss: 0.4407 Validation Loss: 0.7705\n",
      "Epoch: 3/3 Step: 97500 Training Loss: 0.0006 Validation Loss: 0.8224\n",
      "Epoch: 3/3 Step: 98000 Training Loss: 0.1730 Validation Loss: 0.8041\n",
      "Epoch: 3/3 Step: 98500 Training Loss: 0.0004 Validation Loss: 0.9488\n",
      "Epoch: 3/3 Step: 99000 Training Loss: 0.0000 Validation Loss: 0.8359\n",
      "Epoch: 3/3 Step: 99500 Training Loss: 0.0000 Validation Loss: 0.8594\n",
      "Epoch: 3/3 Step: 100000 Training Loss: 0.5663 Validation Loss: 0.7268\n",
      "Epoch: 3/3 Step: 100500 Training Loss: 0.1758 Validation Loss: 0.8032\n",
      "Epoch: 3/3 Step: 101000 Training Loss: 0.0105 Validation Loss: 0.7691\n",
      "Epoch: 3/3 Step: 101500 Training Loss: 0.0495 Validation Loss: 0.8235\n",
      "Epoch: 3/3 Step: 102000 Training Loss: 0.0000 Validation Loss: 0.8809\n",
      "Epoch: 3/3 Step: 102500 Training Loss: 0.0005 Validation Loss: 0.7452\n",
      "Epoch: 3/3 Step: 103000 Training Loss: 0.0479 Validation Loss: 0.7865\n",
      "Epoch: 3/3 Step: 103500 Training Loss: 0.2262 Validation Loss: 0.9424\n",
      "Epoch: 3/3 Step: 104000 Training Loss: 0.0003 Validation Loss: 0.8199\n",
      "Epoch: 3/3 Step: 104500 Training Loss: 0.0066 Validation Loss: 0.8213\n",
      "Epoch: 3/3 Step: 105000 Training Loss: 0.0406 Validation Loss: 0.7943\n",
      "Epoch: 3/3 Step: 105500 Training Loss: 0.0000 Validation Loss: 0.8863\n",
      "Epoch: 3/3 Step: 106000 Training Loss: 0.0000 Validation Loss: 0.7820\n",
      "Epoch: 3/3 Step: 106500 Training Loss: 0.1193 Validation Loss: 0.7124\n",
      "Epoch: 3/3 Step: 107000 Training Loss: 0.0000 Validation Loss: 0.8268\n",
      "Epoch: 3/3 Step: 107500 Training Loss: 0.0494 Validation Loss: 0.8269\n",
      "Epoch: 3/3 Step: 108000 Training Loss: 0.0164 Validation Loss: 0.7666\n",
      "Epoch: 3/3 Step: 108500 Training Loss: 0.0000 Validation Loss: 0.7660\n",
      "Epoch: 3/3 Step: 109000 Training Loss: 0.1702 Validation Loss: 0.7898\n",
      "Epoch: 3/3 Step: 109500 Training Loss: 2.0974 Validation Loss: 0.8122\n",
      "Epoch: 3/3 Step: 110000 Training Loss: 1.0641 Validation Loss: 0.7840\n",
      "Epoch: 3/3 Step: 110500 Training Loss: 2.4811 Validation Loss: 0.7776\n",
      "Epoch: 3/3 Step: 111000 Training Loss: 0.0000 Validation Loss: 0.7427\n",
      "Epoch: 3/3 Step: 111500 Training Loss: 0.6863 Validation Loss: 0.7592\n",
      "Epoch: 3/3 Step: 112000 Training Loss: 0.0081 Validation Loss: 0.7870\n",
      "Epoch: 3/3 Step: 112500 Training Loss: 0.0742 Validation Loss: 0.9922\n",
      "Epoch: 3/3 Step: 113000 Training Loss: 0.0640 Validation Loss: 0.7808\n",
      "Epoch: 3/3 Step: 113500 Training Loss: 0.3712 Validation Loss: 0.8281\n",
      "Epoch: 3/3 Step: 114000 Training Loss: 0.0501 Validation Loss: 0.8688\n",
      "Epoch: 3/3 Step: 114500 Training Loss: 0.1948 Validation Loss: 0.7972\n",
      "Epoch: 3/3 Step: 115000 Training Loss: 0.0083 Validation Loss: 0.8502\n",
      "Epoch: 3/3 Step: 115500 Training Loss: 0.9303 Validation Loss: 0.7978\n",
      "Epoch: 3/3 Step: 116000 Training Loss: 0.0512 Validation Loss: 0.8275\n",
      "Epoch: 3/3 Step: 116500 Training Loss: 6.1267 Validation Loss: 0.8027\n",
      "Epoch: 3/3 Step: 117000 Training Loss: 0.0513 Validation Loss: 0.7705\n",
      "Epoch: 3/3 Step: 117500 Training Loss: 0.0000 Validation Loss: 0.8274\n",
      "Epoch: 3/3 Step: 118000 Training Loss: 0.5606 Validation Loss: 0.8009\n",
      "Epoch: 3/3 Step: 118500 Training Loss: 9.9272 Validation Loss: 0.8271\n",
      "Epoch: 3/3 Step: 119000 Training Loss: 0.0005 Validation Loss: 0.8174\n",
      "Epoch: 3/3 Step: 119500 Training Loss: 0.0472 Validation Loss: 0.8552\n",
      "Epoch: 3/3 Step: 120000 Training Loss: 1.0504 Validation Loss: 0.7970\n",
      "Epoch: 3/3 Step: 120500 Training Loss: 0.5606 Validation Loss: 0.7232\n",
      "Epoch: 3/3 Step: 121000 Training Loss: 0.0013 Validation Loss: 0.7930\n",
      "Epoch: 3/3 Step: 121500 Training Loss: 0.0849 Validation Loss: 0.8085\n",
      "Epoch: 3/3 Step: 122000 Training Loss: 0.0001 Validation Loss: 0.8327\n",
      "Epoch: 3/3 Step: 122500 Training Loss: 1.4750 Validation Loss: 0.8043\n",
      "Epoch: 3/3 Step: 123000 Training Loss: 0.2178 Validation Loss: 0.8391\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    h = net.init_hidden(batch_size)\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        step += 1  \n",
    "        net.zero_grad()\n",
    "        output, h = net(inputs)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (step % print_every) == 0:            \n",
    "            net.eval()\n",
    "            valid_losses = []\n",
    "\n",
    "            for v_inputs, v_labels in valid_loader:\n",
    "                       \n",
    "                v_output, v_h = net(v_inputs)\n",
    "                v_loss = criterion(v_output, v_labels)\n",
    "                valid_losses.append(v_loss.item())\n",
    "\n",
    "            print(\"Epoch: {}/{}\".format((epoch+1), n_epochs),\n",
    "                  \"Step: {}\".format(step),\n",
    "                  \"Training Loss: {:.4f}\".format(loss.item()),\n",
    "                  \"Validation Loss: {:.4f}\".format(np.mean(valid_losses)))\n",
    "            net.train()"
   ]
  },
  {
   "source": [
    "Next steps will include evaluating the model on test data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd0f9f730add346c93043ac242f059f9df8afba4a981fd3a622ff9f4a14c739e6d6",
   "display_name": "Python 3.8.5 64-bit ('nlp_2': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}