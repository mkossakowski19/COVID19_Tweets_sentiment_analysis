{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ca7f3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import re\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ba591cb",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Size of the train dataset: (41157, 6)\nSize of the test dataset: (3798, 6)\n"
     ]
    }
   ],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "ENGLISH_WORDS = set(words.words())\n",
    "df_train = pd.read_csv(r\"data\\Corona_NLP_train.csv\", encoding='latin1')\n",
    "df_test = pd.read_csv(r\"data\\Corona_NLP_test.csv\", encoding='latin1')\n",
    "\n",
    "print(\"Size of the train dataset: {}\".format(df_train.shape))\n",
    "print(\"Size of the test dataset: {}\".format(df_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b82416",
   "metadata": {},
   "source": [
    "I'm defining preprocessing functions from previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13549418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recode_sentiment(y):\n",
    "\n",
    "    if y in ['Extremely Positive', 'Positive']:\n",
    "        return 'Positive'\n",
    "    elif y in ['Extremely Negative', 'Negative']:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "def remove_url(string):\n",
    "    return re.sub(r'https?://\\S+|www\\.\\S+', '', string)\n",
    "\n",
    "def remove_html(string):\n",
    "    return re.sub(r'<.*?>', '', string)\n",
    "\n",
    "def remove_numbers(string):\n",
    "    return re.sub(r'\\d+', '', string)\n",
    "\n",
    "def remove_mentions(string):\n",
    "    return re.sub(r'@\\w+', '', string)\n",
    "\n",
    "def remove_hashtags(string):\n",
    "    return re.sub(r'#\\w+', '', string)\n",
    "\n",
    "def clean_data(tweet, return_tokenized=True):\n",
    "    \n",
    "    # Tokenization\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    cleaned_tweet = []\n",
    "    \n",
    "    for token, tag in pos_tag(tokens):\n",
    "        \n",
    "        # Cleaning tokens with regular expressions\n",
    "        token = remove_url(token)\n",
    "        token = remove_html(token)\n",
    "        token = remove_numbers(token)\n",
    "        token = remove_mentions(token)\n",
    "        token = remove_hashtags(token)\n",
    "        \n",
    "        # Lemmatizing tokens with part of speech recognition\n",
    "        \n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        \n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "        \n",
    "        token = token.lower()\n",
    "        \n",
    "        if token not in punctuation and token not in STOPWORDS and token in ENGLISH_WORDS:\n",
    "            cleaned_tweet.append(token)\n",
    "    #TfidfVectorizer accepts strings instead of lists of tokens\n",
    "    if not return_tokenized:\n",
    "        cleaned_tweet = ' '.join([token for token in cleaned_tweet])\n",
    "\n",
    "    return cleaned_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04e4b7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['OriginalTweet'], df_test['OriginalTweet'] = \\\n",
    "    df_train['OriginalTweet'].apply(lambda x: clean_data(x, return_tokenized=True)),\\\n",
    "    df_test['OriginalTweet'].apply(lambda x: clean_data(x, return_tokenized=True))\n",
    "\n",
    "df_train['Sentiment'], df_test['Sentiment'] = \\\n",
    "    df_train['Sentiment'].apply(recode_sentiment), df_test['Sentiment'].apply(recode_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f88747c",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   UserName  ScreenName                   Location     TweetAt  \\\n",
       "0      3799       48751                     London  16-03-2020   \n",
       "1      3800       48752                         UK  16-03-2020   \n",
       "2      3801       48753                  Vagabonds  16-03-2020   \n",
       "3      3802       48754                        NaN  16-03-2020   \n",
       "4      3803       48755                        NaN  16-03-2020   \n",
       "5      3804       48756  ÃT: 36.319708,-82.363649  16-03-2020   \n",
       "6      3805       48757       35.926541,-78.753267  16-03-2020   \n",
       "7      3806       48758                    Austria  16-03-2020   \n",
       "8      3807       48759            Atlanta, GA USA  16-03-2020   \n",
       "9      3808       48760           BHAVNAGAR,GUJRAT  16-03-2020   \n",
       "\n",
       "                                       OriginalTweet Sentiment  \n",
       "0                                                 []   Neutral  \n",
       "1  [advice, talk, family, exchange, phone, number...  Positive  \n",
       "2  [give, elderly, disable, dedicate, shopping, h...  Positive  \n",
       "3  [food, stock, one, empty, please, panic, enoug...  Positive  \n",
       "4  [ready, go, supermarket, outbreak, paranoid, f...  Negative  \n",
       "5  [news, first, confirm, covid, case, come, coun...  Positive  \n",
       "6  [cashier, grocery, store, share, insight, prov...  Positive  \n",
       "7           [supermarket, today, buy, toilet, paper]   Neutral  \n",
       "8  [due, covid, retail, store, classroom, open, b...  Positive  \n",
       "9  [corona, prevention, stop, buy, thing, cash, u...  Negative  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>UserName</th>\n      <th>ScreenName</th>\n      <th>Location</th>\n      <th>TweetAt</th>\n      <th>OriginalTweet</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3799</td>\n      <td>48751</td>\n      <td>London</td>\n      <td>16-03-2020</td>\n      <td>[]</td>\n      <td>Neutral</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3800</td>\n      <td>48752</td>\n      <td>UK</td>\n      <td>16-03-2020</td>\n      <td>[advice, talk, family, exchange, phone, number...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3801</td>\n      <td>48753</td>\n      <td>Vagabonds</td>\n      <td>16-03-2020</td>\n      <td>[give, elderly, disable, dedicate, shopping, h...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3802</td>\n      <td>48754</td>\n      <td>NaN</td>\n      <td>16-03-2020</td>\n      <td>[food, stock, one, empty, please, panic, enoug...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3803</td>\n      <td>48755</td>\n      <td>NaN</td>\n      <td>16-03-2020</td>\n      <td>[ready, go, supermarket, outbreak, paranoid, f...</td>\n      <td>Negative</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3804</td>\n      <td>48756</td>\n      <td>ÃT: 36.319708,-82.363649</td>\n      <td>16-03-2020</td>\n      <td>[news, first, confirm, covid, case, come, coun...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>3805</td>\n      <td>48757</td>\n      <td>35.926541,-78.753267</td>\n      <td>16-03-2020</td>\n      <td>[cashier, grocery, store, share, insight, prov...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>3806</td>\n      <td>48758</td>\n      <td>Austria</td>\n      <td>16-03-2020</td>\n      <td>[supermarket, today, buy, toilet, paper]</td>\n      <td>Neutral</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>3807</td>\n      <td>48759</td>\n      <td>Atlanta, GA USA</td>\n      <td>16-03-2020</td>\n      <td>[due, covid, retail, store, classroom, open, b...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>3808</td>\n      <td>48760</td>\n      <td>BHAVNAGAR,GUJRAT</td>\n      <td>16-03-2020</td>\n      <td>[corona, prevention, stop, buy, thing, cash, u...</td>\n      <td>Negative</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc6950da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['TweetLen'], df_test['TweetLen'] = \\\n",
    "    df_train['OriginalTweet'].apply(lambda x: len(x)), df_test['OriginalTweet'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ddd9ffc",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   UserName  ScreenName                   Location     TweetAt  \\\n",
       "0      3799       48751                     London  16-03-2020   \n",
       "1      3800       48752                         UK  16-03-2020   \n",
       "2      3801       48753                  Vagabonds  16-03-2020   \n",
       "3      3802       48754                        NaN  16-03-2020   \n",
       "4      3803       48755                        NaN  16-03-2020   \n",
       "5      3804       48756  ÃT: 36.319708,-82.363649  16-03-2020   \n",
       "6      3805       48757       35.926541,-78.753267  16-03-2020   \n",
       "7      3806       48758                    Austria  16-03-2020   \n",
       "8      3807       48759            Atlanta, GA USA  16-03-2020   \n",
       "9      3808       48760           BHAVNAGAR,GUJRAT  16-03-2020   \n",
       "\n",
       "                                       OriginalTweet Sentiment  TweetLen  \n",
       "0                                                 []   Neutral         0  \n",
       "1  [advice, talk, family, exchange, phone, number...  Positive        22  \n",
       "2  [give, elderly, disable, dedicate, shopping, h...  Positive         9  \n",
       "3  [food, stock, one, empty, please, panic, enoug...  Positive        15  \n",
       "4  [ready, go, supermarket, outbreak, paranoid, f...  Negative        14  \n",
       "5  [news, first, confirm, covid, case, come, coun...  Positive        22  \n",
       "6  [cashier, grocery, store, share, insight, prov...  Positive        12  \n",
       "7           [supermarket, today, buy, toilet, paper]   Neutral         5  \n",
       "8  [due, covid, retail, store, classroom, open, b...  Positive        20  \n",
       "9  [corona, prevention, stop, buy, thing, cash, u...  Negative        19  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>UserName</th>\n      <th>ScreenName</th>\n      <th>Location</th>\n      <th>TweetAt</th>\n      <th>OriginalTweet</th>\n      <th>Sentiment</th>\n      <th>TweetLen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3799</td>\n      <td>48751</td>\n      <td>London</td>\n      <td>16-03-2020</td>\n      <td>[]</td>\n      <td>Neutral</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3800</td>\n      <td>48752</td>\n      <td>UK</td>\n      <td>16-03-2020</td>\n      <td>[advice, talk, family, exchange, phone, number...</td>\n      <td>Positive</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3801</td>\n      <td>48753</td>\n      <td>Vagabonds</td>\n      <td>16-03-2020</td>\n      <td>[give, elderly, disable, dedicate, shopping, h...</td>\n      <td>Positive</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3802</td>\n      <td>48754</td>\n      <td>NaN</td>\n      <td>16-03-2020</td>\n      <td>[food, stock, one, empty, please, panic, enoug...</td>\n      <td>Positive</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3803</td>\n      <td>48755</td>\n      <td>NaN</td>\n      <td>16-03-2020</td>\n      <td>[ready, go, supermarket, outbreak, paranoid, f...</td>\n      <td>Negative</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3804</td>\n      <td>48756</td>\n      <td>ÃT: 36.319708,-82.363649</td>\n      <td>16-03-2020</td>\n      <td>[news, first, confirm, covid, case, come, coun...</td>\n      <td>Positive</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>3805</td>\n      <td>48757</td>\n      <td>35.926541,-78.753267</td>\n      <td>16-03-2020</td>\n      <td>[cashier, grocery, store, share, insight, prov...</td>\n      <td>Positive</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>3806</td>\n      <td>48758</td>\n      <td>Austria</td>\n      <td>16-03-2020</td>\n      <td>[supermarket, today, buy, toilet, paper]</td>\n      <td>Neutral</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>3807</td>\n      <td>48759</td>\n      <td>Atlanta, GA USA</td>\n      <td>16-03-2020</td>\n      <td>[due, covid, retail, store, classroom, open, b...</td>\n      <td>Positive</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>3808</td>\n      <td>48760</td>\n      <td>BHAVNAGAR,GUJRAT</td>\n      <td>16-03-2020</td>\n      <td>[corona, prevention, stop, buy, thing, cash, u...</td>\n      <td>Negative</td>\n      <td>19</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80d23648",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = \\\n",
    "    df_train.loc[df_train['TweetLen'] > 0,], df_test.loc[df_test['TweetLen'] > 0,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "819e73f7",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    UserName  ScreenName                   Location     TweetAt  \\\n",
       "1       3800       48752                         UK  16-03-2020   \n",
       "2       3801       48753                  Vagabonds  16-03-2020   \n",
       "3       3802       48754                        NaN  16-03-2020   \n",
       "4       3803       48755                        NaN  16-03-2020   \n",
       "5       3804       48756  ÃT: 36.319708,-82.363649  16-03-2020   \n",
       "6       3805       48757       35.926541,-78.753267  16-03-2020   \n",
       "7       3806       48758                    Austria  16-03-2020   \n",
       "8       3807       48759            Atlanta, GA USA  16-03-2020   \n",
       "9       3808       48760           BHAVNAGAR,GUJRAT  16-03-2020   \n",
       "10      3809       48761             Makati, Manila  16-03-2020   \n",
       "\n",
       "                                        OriginalTweet Sentiment  TweetLen  \n",
       "1   [advice, talk, family, exchange, phone, number...  Positive        22  \n",
       "2   [give, elderly, disable, dedicate, shopping, h...  Positive         9  \n",
       "3   [food, stock, one, empty, please, panic, enoug...  Positive        15  \n",
       "4   [ready, go, supermarket, outbreak, paranoid, f...  Negative        14  \n",
       "5   [news, first, confirm, covid, case, come, coun...  Positive        22  \n",
       "6   [cashier, grocery, store, share, insight, prov...  Positive        12  \n",
       "7            [supermarket, today, buy, toilet, paper]   Neutral         5  \n",
       "8   [due, covid, retail, store, classroom, open, b...  Positive        20  \n",
       "9   [corona, prevention, stop, buy, thing, cash, u...  Negative        19  \n",
       "10  [month, crowd, supermarket, restaurant, howeve...   Neutral        16  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>UserName</th>\n      <th>ScreenName</th>\n      <th>Location</th>\n      <th>TweetAt</th>\n      <th>OriginalTweet</th>\n      <th>Sentiment</th>\n      <th>TweetLen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>3800</td>\n      <td>48752</td>\n      <td>UK</td>\n      <td>16-03-2020</td>\n      <td>[advice, talk, family, exchange, phone, number...</td>\n      <td>Positive</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3801</td>\n      <td>48753</td>\n      <td>Vagabonds</td>\n      <td>16-03-2020</td>\n      <td>[give, elderly, disable, dedicate, shopping, h...</td>\n      <td>Positive</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3802</td>\n      <td>48754</td>\n      <td>NaN</td>\n      <td>16-03-2020</td>\n      <td>[food, stock, one, empty, please, panic, enoug...</td>\n      <td>Positive</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3803</td>\n      <td>48755</td>\n      <td>NaN</td>\n      <td>16-03-2020</td>\n      <td>[ready, go, supermarket, outbreak, paranoid, f...</td>\n      <td>Negative</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3804</td>\n      <td>48756</td>\n      <td>ÃT: 36.319708,-82.363649</td>\n      <td>16-03-2020</td>\n      <td>[news, first, confirm, covid, case, come, coun...</td>\n      <td>Positive</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>3805</td>\n      <td>48757</td>\n      <td>35.926541,-78.753267</td>\n      <td>16-03-2020</td>\n      <td>[cashier, grocery, store, share, insight, prov...</td>\n      <td>Positive</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>3806</td>\n      <td>48758</td>\n      <td>Austria</td>\n      <td>16-03-2020</td>\n      <td>[supermarket, today, buy, toilet, paper]</td>\n      <td>Neutral</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>3807</td>\n      <td>48759</td>\n      <td>Atlanta, GA USA</td>\n      <td>16-03-2020</td>\n      <td>[due, covid, retail, store, classroom, open, b...</td>\n      <td>Positive</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>3808</td>\n      <td>48760</td>\n      <td>BHAVNAGAR,GUJRAT</td>\n      <td>16-03-2020</td>\n      <td>[corona, prevention, stop, buy, thing, cash, u...</td>\n      <td>Negative</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>3809</td>\n      <td>48761</td>\n      <td>Makati, Manila</td>\n      <td>16-03-2020</td>\n      <td>[month, crowd, supermarket, restaurant, howeve...</td>\n      <td>Neutral</td>\n      <td>16</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "840d2c73",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Size of the train dataset: (41052, 7)\nSize of the test dataset: (3792, 7)\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of the train dataset: {}\".format(df_train.shape))\n",
    "print(\"Size of the test dataset: {}\".format(df_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91e196c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(data):\n",
    "    all_words = []\n",
    "    for x in data:\n",
    "        for token in x:\n",
    "            all_words.append(token)\n",
    "    \n",
    "    return set(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f497cef2",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocabulary length: 12962\n"
     ]
    }
   ],
   "source": [
    "vocab = create_corpus(df_train['OriginalTweet'].values)\n",
    "print('Vocabulary length: {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionaries(vocab):\n",
    "\n",
    "    word_to_int_dict = {w:i+1 for i, w in enumerate(vocab)}\n",
    "    int_to_word_dict = {i:w for w, i in word_to_int_dict.items()}\n",
    "\n",
    "    word_to_int_dict[''] = 0\n",
    "    int_to_word_dict[0] = ''\n",
    "\n",
    "    return word_to_int_dict, int_to_word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_int_dict, int_to_word_dict = create_dictionaries(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(sequence, target_len=25):\n",
    "\n",
    "    padded_sequence = sequence.copy()\n",
    "\n",
    "    length = len(padded_sequence)\n",
    "\n",
    "    if length > target_len:\n",
    "        padded_sequence = padded_sequence[:target_len]\n",
    "    elif length < target_len:\n",
    "        while length < target_len:\n",
    "            padded_sequence.append('')\n",
    "            length += 1\n",
    "    \n",
    "    return padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "count    41052.000000\n",
       "mean        13.420150\n",
       "std          6.091071\n",
       "min          1.000000\n",
       "25%          8.000000\n",
       "50%         14.000000\n",
       "75%         18.000000\n",
       "max         34.000000\n",
       "Name: TweetLen, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "df_train['TweetLen'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['OriginalTweet'], df_test['OriginalTweet'] = df_train['OriginalTweet'].apply(pad_sequence), df_test['OriginalTweet'].apply(pad_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "count    41052.0\n",
       "mean        25.0\n",
       "std          0.0\n",
       "min         25.0\n",
       "25%         25.0\n",
       "50%         25.0\n",
       "75%         25.0\n",
       "max         25.0\n",
       "Name: TweetLen, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "df_train['TweetLen'], df_test['TweetLen'] = \\\n",
    "    df_train['OriginalTweet'].apply(lambda x: len(x)), df_test['OriginalTweet'].apply(lambda x: len(x))\n",
    "df_train['TweetLen'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sequence(text):\n",
    "\n",
    "    encoded_sequence = np.array([word_to_int_dict[word] if word in word_to_int_dict.keys() else word_to_int_dict[''] for word in text])\n",
    "    return encoded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original sequence: ['advice', 'talk', 'family', 'exchange', 'phone', 'number', 'create', 'contact', 'list', 'phone', 'number', 'school', 'employer', 'chemist', 'set', 'shopping', 'account', 'poss', 'adequate', 'supply', 'regular', 'order', '', '', '']\n\n--------------------------------------------------\nEncoded sequence: [ 5870 10611 10080  2440  1040  5313  1924  7415  5473  1040  5313  3500\n 12139 12023  7079  2971  4899  5428  2532  6961  3694  7038     0     0\n     0]\n"
     ]
    }
   ],
   "source": [
    "print('Original sequence: {}'.format(df_train['OriginalTweet'].iloc[0]))\n",
    "print()\n",
    "print('--'*25)\n",
    "print('Encoded sequence: {}'.format(encode_sequence(df_train['OriginalTweet'].iloc[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mapping_dict = {'Negative':0, 'Neutral':1, 'Positive':2}\n",
    "\n",
    "x_train = []\n",
    "\n",
    "for x in df_train['OriginalTweet'].values:\n",
    "    x_train.append(encode_sequence(x))\n",
    "x_train = np.array(x_train)\n",
    "\n",
    "y_train = df_train['Sentiment'].map(y_mapping_dict).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x_train shape: (41052, 25)\ny_train shape: (41052,)\n"
     ]
    }
   ],
   "source": [
    "print('x_train shape: {}'.format(x_train.shape))\n",
    "print('y_train shape: {}'.format(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_idx_border = int(df_test.shape[0] / 2)\n",
    "\n",
    "x_valid = []\n",
    "x_test = []\n",
    "\n",
    "for x in df_test['OriginalTweet'].values[:valid_idx_border]:\n",
    "    x_valid.append(encode_sequence(x))\n",
    "\n",
    "for x in df_test['OriginalTweet'].values[valid_idx_border:]:\n",
    "    x_test.append(encode_sequence(x))\n",
    "\n",
    "x_valid = np.array(x_valid)\n",
    "x_test = np.array(x_test)\n",
    "\n",
    "y_valid = df_test['Sentiment'].map(y_mapping_dict).values[:valid_idx_border]\n",
    "y_test = df_test['Sentiment'].map(y_mapping_dict).values[valid_idx_border:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x_valid shape: (1896, 25)\ny_valid shape: (1896,)\n--------------------------------------------------\nx_test shape: (1896, 25)\ny_test shape: (1896,)\n"
     ]
    }
   ],
   "source": [
    "print('x_valid shape: {}'.format(x_valid.shape))\n",
    "print('y_valid shape: {}'.format(y_valid.shape))\n",
    "print('-'*50)\n",
    "print('x_test shape: {}'.format(x_test.shape))\n",
    "print('y_test shape: {}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[3667 9354 9447  514 6767    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0]\n--------------------------------------------------\n[ 3878  2945 11380  1287  9935  1287  1773  9048  5401  1287  7804  8295\n  1287  3413     0     0     0     0     0     0     0     0     0     0\n     0]\n"
     ]
    }
   ],
   "source": [
    "print(x_valid[-1])\n",
    "print('-'*50)\n",
    "print(x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda:0\n_CudaDeviceProperties(name='GeForce RTX 2070 SUPER', major=7, minor=5, total_memory=8192MB, multi_processor_count=40)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "print(torch.cuda.get_device_properties('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = torch.tensor(x_train, device=device).long(), torch.tensor(y_train, device=device).long()\n",
    "valid_x, valid_y = torch.tensor(x_valid, device=device).long(), torch.tensor(y_valid, device=device).long()\n",
    "test_x, test_y = torch.tensor(x_test, device=device).long(), torch.tensor(y_test, device=device).long()\n",
    "\n",
    "train_data = TensorDataset(train_x, train_y)\n",
    "valid_data = TensorDataset(valid_x, valid_y)\n",
    "test_data = TensorDataset(test_x, test_y)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sample input: \ntensor([[ 7476,  6487,  7079, 11919,  3594,  7536,  4166,  1053, 12913,  6808,\n          3244,  1985,   382,  2945, 10033,  7278,  6746,  1385,  7294,  4208,\n             0,     0,     0,     0,     0],\n        [ 6497,  3317,  9447,   514, 11140,  1287,  9935,  2971, 10499,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0],\n        [ 3196,  1689,  2948, 12229,  8348, 12819,  9079,  1041,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0],\n        [ 8295, 11019,   758, 11040, 10891,  2274,  7093, 12209,  6741,  2529,\n         11412, 10803,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0],\n        [ 3594,  3631, 10617,  7161,  2153,  3002, 10705,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0],\n        [  382,  9447,  6447, 12819,  2724, 11798,   514, 11670,   673,  4826,\n          5127, 11670,   823, 12523,  2724,  7795,  2971,     0,     0,     0,\n             0,     0,     0,     0,     0],\n        [ 1950,   511,  4208,  6324,   514, 12879,   837, 11380,  8265,  2945,\n          9080,  7207,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0],\n        [ 5838,  7866,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0],\n        [ 1035,  2488,   823,  7371,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0],\n        [ 3597, 11422,  5224,  7162,   179, 12225,  5838,  1449,  2488,  5658,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0],\n        [11476,  5254,   967,  6762,  2745,  2727,  1317,  7838, 10224,  1531,\n          7813,  3013,  7639,  7246,  4670, 11136, 12909,     0,     0,     0,\n             0,     0,     0,     0,     0],\n        [ 8153,  4049,   414,  3273,  1921, 11352,  1705,  7374,  6313,  9552,\n          2654, 10708,  3597,  6324,  4328, 12209,  7374, 12258,  6301,  3273,\n          4328, 12209,  6620, 12510,  6301],\n        [ 9727,  4707,   414, 12539, 10724, 12913, 10169,  7711,  3330, 10753,\n         11981,  8295,  5501,  5247,  8649,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0],\n        [ 9525,  1985,  2701,  1498,  5911,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0],\n        [ 6132,  9153,  7189,  9727,  7727,  9935,  4834,  5146,  6324,  5936,\n          2945,  2235,  8991,  8291, 12460,  2765, 11589,  7570,  8007,     0,\n             0,     0,     0,     0,     0],\n        [ 3113, 10444,  7678,  8348,  5127, 10812,  9749, 12321, 10080,  8153,\n          4941,  8045,   783, 10080, 12505, 10252,  2271,  9935,  9287,   514,\n          5854, 11975,  6324, 10841,  7825],\n        [  997,  9727,  6542,  1385,  8295,  6324,  4328,  5501, 12129,  5438,\n          8649, 10147,  1606,  1166,  7341,  2945,  1985,  1287,  9935,     0,\n             0,     0,     0,     0,     0],\n        [  679, 10611,  6056,   915,  4728,  1985,  9727, 10705,  4532,  3594,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0],\n        [   72,  1675,  8371,  8295,  1399,  6046,  9727,  3308,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0],\n        [ 3594,  6324,  1399,  1385,  5827,  2945,  8943, 11075,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0],\n        [ 5509,  4328,  6357,  8274,  7836,  9447,   514,  5616,  2488,  5509,\n          7200,  6595, 12183,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0],\n        [ 2945,   329,  1185,  9287, 10611,  9727,  3594,  5668,  4208,  8465,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0],\n        [10812,  9749,  2904, 10033,  6512,  5838, 11864, 11839,  5802,   227,\n          6587,  5691,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0],\n        [ 9705,  8553,  8136,  5501,  4684,  8576,  7660,  2362,  7142, 11823,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0],\n        [ 4684,  9813,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0],\n        [10781,  3667, 12209,  9447,   514,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0],\n        [ 7935,  5674,  3791,  2126,  3306,  9488, 12502, 12909,  2362,  6961,\n          6384,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0],\n        [10490,  5165,  1617,  7273, 11635,   700, 12209,  6324,  8295,  8435,\n          5366,   975,  1608,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0],\n        [11424,  8489,  7634,  3036,  3401,   102,  5167,  7496, 10724,  8413,\n          9651,  7189,  8466,   435,  6324,  9080,  3401, 11500,  1809,  4911,\n          6726,  7787,     0,     0,     0],\n        [ 9727,  3781,  8400,  4328,  8958,  9373,  6324, 11399,  6022,  3881,\n          2645,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0],\n        [12728,  1985,  6132,  1176, 10062,  5205,  5563, 10837,   190,  4457,\n            41,  5661,  6991,  5838,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0],\n        [ 6497,  2765,  8295,  8931,  5802,  3165,  4035,   227,  4130,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0]], device='cuda:0')\ntorch.Size([32, 25])\nSample label: \ntensor([0, 0, 0, 1, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 2, 2, 1, 0,\n        1, 1, 2, 2, 2, 1, 2, 1], device='cuda:0')\ntorch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "data_iter = iter(train_loader)\n",
    "sample_x, sample_y = next(data_iter)\n",
    "\n",
    "print('Sample input: ')\n",
    "print(sample_x)\n",
    "print(sample_x.size())\n",
    "print('Sample label: ')\n",
    "print(sample_y)\n",
    "print(sample_y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, n_vocab, n_embed, n_hidden, n_output, n_layers, drop_p = 0.8):\n",
    "\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "\n",
    "        self.n_vocab = n_vocab\n",
    "        self.n_embed = n_embed\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        self.n_layers = n_layers\n",
    "        self.drop_p = drop_p\n",
    "\n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        self.lstm = nn.LSTM(n_embed, n_hidden, n_layers, batch_first = True, dropout = drop_p)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.fc = nn.Linear(n_hidden, n_output)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        batchsize = x.shape[0]\n",
    "        embedded_words = self.embedding(x)\n",
    "        lstm_out, h = self.lstm(embedded_words)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        fc_out = self.fc(lstm_out)\n",
    "        softmax_out = self.softmax(fc_out)\n",
    "        softmax_out = softmax_out.view(batchsize, -1)\n",
    "        softmax_last_three = softmax_out[:, -3:]\n",
    "\n",
    "        return softmax_last_three, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "\n",
    "        device = torch.device('cuda:0')\n",
    "        weights = next(self.parameters()).data\n",
    "        h = (weights.new(self.n_layers, batch_size,\\\n",
    "        self.n_hidden).zero_().to(device),\\\n",
    "        weights.new(self.n_layers, batch_size,\\\n",
    "        self.n_hidden).zero_().to(device))\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "SentimentLSTM(\n",
       "  (embedding): Embedding(12963, 100)\n",
       "  (lstm): LSTM(100, 50, num_layers=2, batch_first=True, dropout=0.8)\n",
       "  (dropout): Dropout(p=0.8, inplace=False)\n",
       "  (fc): Linear(in_features=50, out_features=3, bias=True)\n",
       "  (softmax): LogSoftmax(dim=-1)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 106
    }
   ],
   "source": [
    "n_vocab = len(word_to_int_dict)\n",
    "n_embed = 100\n",
    "n_hidden = 50\n",
    "n_output = 3\n",
    "n_layers = 2\n",
    "\n",
    "net = SentimentLSTM(n_vocab, n_embed, n_hidden, n_output, n_layers)\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every = 500\n",
    "step = 0\n",
    "n_epochs = 4\n",
    "clip = 5 \n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.RMSprop(net.parameters(), lr = 0.0015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1/4 Step: 500 Training Loss: 1.0139 Validation Loss: 0.9363\n",
      "Epoch: 1/4 Step: 1000 Training Loss: 0.8810 Validation Loss: 0.9024\n",
      "Epoch: 2/4 Step: 1500 Training Loss: 0.8945 Validation Loss: 0.8565\n",
      "Epoch: 2/4 Step: 2000 Training Loss: 0.7109 Validation Loss: 0.7421\n",
      "Epoch: 2/4 Step: 2500 Training Loss: 1.0284 Validation Loss: 0.6893\n",
      "Epoch: 3/4 Step: 3000 Training Loss: 0.6937 Validation Loss: 0.6832\n",
      "Epoch: 3/4 Step: 3500 Training Loss: 0.3433 Validation Loss: 0.6529\n",
      "Epoch: 4/4 Step: 4000 Training Loss: 0.7705 Validation Loss: 0.6422\n",
      "Epoch: 4/4 Step: 4500 Training Loss: 0.4258 Validation Loss: 0.6327\n",
      "Epoch: 4/4 Step: 5000 Training Loss: 0.5154 Validation Loss: 0.6019\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    h = net.init_hidden(batch_size)\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        step += 1  \n",
    "        net.zero_grad()\n",
    "        output, h = net(inputs)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (step % print_every) == 0:            \n",
    "            net.eval()\n",
    "            valid_losses = []\n",
    "\n",
    "            for v_inputs, v_labels in valid_loader:\n",
    "                       \n",
    "                v_output, v_h = net(v_inputs)\n",
    "                v_loss = criterion(v_output, v_labels)\n",
    "                valid_losses.append(v_loss.item())\n",
    "\n",
    "            print(\"Epoch: {}/{}\".format((epoch+1), n_epochs),\n",
    "                  \"Step: {}\".format(step),\n",
    "                  \"Training Loss: {:.4f}\".format(loss.item()),\n",
    "                  \"Validation Loss: {:.4f}\".format(np.mean(valid_losses)))\n",
    "            net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, x):\n",
    "    \n",
    "    net.eval()\n",
    "    preds, h = net(x)\n",
    "    preds = preds.cpu()\n",
    "    preds = preds.detach().numpy()\n",
    "    classes = []\n",
    "    for i in preds:\n",
    "        classes.append(np.argmax(i))\n",
    "    return np.array(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training accuracy: 0.8581555100847705\nValidation accuracy: 0.7747890295358649\nTest accuracy: 0.7816455696202531\n"
     ]
    }
   ],
   "source": [
    "preds_train = predict(net, train_x)\n",
    "preds_valid = predict(net, valid_x)\n",
    "preds_test = predict(net, test_x)\n",
    "\n",
    "print('Training accuracy: {}'.format(accuracy_score(y_train, preds_train)))\n",
    "print('Validation accuracy: {}'.format(accuracy_score(y_valid, preds_valid)))\n",
    "print('Test accuracy: {}'.format(accuracy_score(y_test, preds_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('nlp_2': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "f9f730add346c93043ac242f059f9df8afba4a981fd3a622ff9f4a14c739e6d6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}